#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ ì œëª©, ì‘ì„±ì, ë³¸ë¬¸ë§Œ ì •í™•íˆ ì¶”ì¶œ
"""

import asyncio
import re
import json
import aiohttp
import os
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright
import argparse
from urllib.parse import urljoin, urlparse


class NaverBlogCrawler:
    def __init__(self):
        self.selectors = {
            'title': 'h1, h2, h3, .title, [class*="title"], .se-title, .blog-title',  # ì œëª©
            'author': '.blog_author, .author, [class*="author"]',  # ì‘ì„±ì
            'content': '.se-main-container, .post-content, .content, article, main, .se-component, #postViewArea',  # ë³¸ë¬¸
            'fallback_title': 'title',  # í˜ì´ì§€ ì œëª©ìœ¼ë¡œ ëŒ€ì²´
            'fallback_content': 'p, div[class*="content"], div[class*="post"]'  # ëŒ€ì²´ ë³¸ë¬¸ ì…€ë ‰í„°
        }
    
    async def extract_content(self, page, url):
        """í˜ì´ì§€ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            await page.wait_for_load_state('networkidle', timeout=10000)
            
            # iframe í™•ì¸ ë° ì²˜ë¦¬
            iframe = await page.query_selector('iframe#mainFrame')
            if iframe:
                print("ğŸ“± iframe ê°ì§€ë¨, iframe ë‚´ìš©ìœ¼ë¡œ ì „í™˜")
                frame = await iframe.content_frame()
                if frame:
                    page = frame  # iframe ë‚´ìš©ì„ ì‚¬ìš©
                    await frame.wait_for_load_state('networkidle', timeout=5000)
            
            # í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ í™•ì¸ (ë””ë²„ê¹…)
            all_text = await page.inner_text('body')
            print(f"ğŸ” ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(all_text)}ì")
            if len(all_text) > 200:
                print(f"ğŸ” ì²« 200ì: {all_text[:200]}...")
            else:
                print(f"ğŸ” ì „ì²´ í…ìŠ¤íŠ¸: {all_text}")
            
            # ì œëª© ì¶”ì¶œ
            title = await self._extract_title(page)
            print(f"ğŸ“ ì¶”ì¶œëœ ì œëª©: {title}")
            
            # ì‘ì„±ì ì¶”ì¶œ  
            author = await self._extract_author(page)
            print(f"ğŸ‘¤ ì¶”ì¶œëœ ì‘ì„±ì: {author}")
            
            # ë¸”ë¡œê·¸ ID ìƒì„±
            blog_id = author if author != "ì‘ì„±ì ì—†ìŒ" else "unknown_blog"
            
            # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ í˜¼í•©ëœ ì½˜í…ì¸  ì¶”ì¶œ (ì›ë³¸ ìˆœì„œ ìœ ì§€)
            content = await self._extract_mixed_content(page, blog_id)
            print(f"ğŸ“„ í˜¼í•© ì½˜í…ì¸  ì¶”ì¶œ ì™„ë£Œ")
            
            # ì´ë¯¸ì§€ ê°œìˆ˜ ê³„ì‚° (ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€ë“¤)
            images_dir = Path(f"images/{blog_id}")
            if images_dir.exists():
                image_files = list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.JPG")) + list(images_dir.glob("*.png"))
                image_count = len(image_files)
            else:
                image_count = 0
            
            # ì‘ì„±ì¼ ì¶”ì¶œ (í˜ì´ì§€ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°)
            published_date = await self._extract_date(page)
            
            return {
                'url': url,
                'title': title,
                'author': author,
                'content': content,
                'published_date': published_date,
                'scraped_at': datetime.now().isoformat(),
                'content_length': len(content) if content else 0,
                'word_count': len(content.split()) if content else 0,
                'image_count': image_count
            }
            
        except Exception as e:
            print(f"ì½˜í…ì¸  ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    async def _extract_title(self, page):
        """ì œëª© ì¶”ì¶œ"""
        try:
            # ì²« ë²ˆì§¸ ì‹œë„: ëª¨ë“  ì œëª© ì…€ë ‰í„°ë“¤ ìˆœì°¨ ì‹œë„
            title_selectors = self.selectors['title'].split(', ')
            for selector in title_selectors:
                title_element = await page.query_selector(selector.strip())
                if title_element:
                    title = await title_element.inner_text()
                    if title.strip() and len(title.strip()) > 2:
                        return title.strip()
            
            # ë‘ ë²ˆì§¸ ì‹œë„: í˜ì´ì§€ ì œëª©ì—ì„œ ì¶”ì¶œ
            page_title = await page.title()
            if page_title:
                # "ì œëª© : ë„¤ì´ë²„ ë¸”ë¡œê·¸" í˜•íƒœì—ì„œ ì œëª© ë¶€ë¶„ë§Œ ì¶”ì¶œ
                if ' : ' in page_title:
                    clean_title = page_title.split(' : ')[0].strip()
                elif ' - ' in page_title:
                    clean_title = page_title.split(' - ')[0].strip()
                else:
                    clean_title = page_title.strip()
                
                if clean_title and 'ë„¤ì´ë²„ ë¸”ë¡œê·¸' not in clean_title:
                    return clean_title
            
            # ì„¸ ë²ˆì§¸ ì‹œë„: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ë¸”ë¡ì—ì„œ ì œëª© ì°¾ê¸°
            all_text = await page.inner_text('body')
            lines = all_text.split('\n')
            for line in lines[:20]:  # ì²« 20ì¤„ì—ì„œ ì œëª© ì°¾ê¸°
                line = line.strip()
                if line and len(line) > 5 and len(line) < 100:
                    if 'CCTV' in line or 'ì„¤ì¹˜' in line or 'í›„ê¸°' in line:
                        return line
                        
            return "ì œëª© ì—†ìŒ"
            
        except Exception as e:
            print(f"ì œëª© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"
    
    async def _extract_author(self, page):
        """ì‘ì„±ì ì¶”ì¶œ"""
        try:
            author_element = await page.query_selector(self.selectors['author'])
            if author_element:
                author = await author_element.inner_text()
                return author.strip() if author else "ì‘ì„±ì ì—†ìŒ"
            
            # ëŒ€ì²´ ë°©ë²•: URLì—ì„œ ë¸”ë¡œê±° ID ì¶”ì¶œ
            url = page.url
            if 'blog.naver.com' in url:
                # blog.naver.com/ë¸”ë¡œê±°ID/ê¸€ë²ˆí˜¸ í˜•íƒœì—ì„œ ë¸”ë¡œê±°ID ì¶”ì¶œ
                match = re.search(r'blog\.naver\.com/([^/]+)/', url)
                if match:
                    return match.group(1)
            
            return "ì‘ì„±ì ì—†ìŒ"
            
        except Exception as e:
            print(f"ì‘ì„±ì ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return "ì‘ì„±ì ì¶”ì¶œ ì‹¤íŒ¨"
    
    async def _extract_mixed_content(self, page, blog_id):
        """í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ í˜¼í•©ëœ ì›ë³¸ ìˆœì„œ ê·¸ëŒ€ë¡œ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            content_selectors = [
                '.se-main-container', 
                '.post-content', 
                '.content', 
                'article', 
                'main', 
                '.se-component', 
                '#postViewArea'
            ]
            
            content_container = None
            for selector in content_selectors:
                content_container = await page.query_selector(selector)
                if content_container:
                    print(f"ğŸ“ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ë°œê²¬: {selector}")
                    break
            
            if not content_container:
                # fallback: body ì „ì²´ ì‚¬ìš©
                content_container = await page.query_selector('body')
                print("ğŸ“ fallback: body ì „ì²´ ì‚¬ìš©")
            
            # DOM ìˆœì„œëŒ€ë¡œ ëª¨ë“  ìš”ì†Œ íƒìƒ‰
            mixed_content = []
            await self._traverse_elements(content_container, mixed_content, blog_id, page)
            
            if not mixed_content:
                return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"
            
            # í˜¼í•© ì½˜í…ì¸ ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
            markdown_content = self._convert_to_markdown(mixed_content)
            return markdown_content
            
        except Exception as e:
            print(f"í˜¼í•© ì½˜í…ì¸  ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

    async def _traverse_elements(self, container, mixed_content, blog_id, page):
        """DOM íŠ¸ë¦¬ë¥¼ ìˆœì„œëŒ€ë¡œ íƒìƒ‰í•˜ë©° í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        try:
            # ì»¨í…Œì´ë„ˆ ë‚´ì˜ ëª¨ë“  ì§ì† ìì‹ ìš”ì†Œë“¤ì„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
            children = await container.query_selector_all('> *')
            
            for child in children:
                tag_name = await child.evaluate('element => element.tagName.toLowerCase()')
                
                # ì´ë¯¸ì§€ ì²˜ë¦¬
                if tag_name == 'img':
                    await self._process_image_element(child, mixed_content, blog_id)
                
                # ì´ë¯¸ì§€ë¥¼ í¬í•¨í•œ divë‚˜ ë‹¤ë¥¸ ì»¨í…Œì´ë„ˆ ì²˜ë¦¬
                elif tag_name in ['div', 'span', 'p', 'section']:
                    # ì´ ìš”ì†Œ ì•ˆì— ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
                    img_in_element = await child.query_selector('img')
                    if img_in_element:
                        # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì´ë¯¸ì§€ ì²˜ë¦¬
                        await self._process_image_element(img_in_element, mixed_content, blog_id)
                    else:
                        # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                        await self._process_text_element(child, mixed_content)
                
                # í…ìŠ¤íŠ¸ ìš”ì†Œ ì²˜ë¦¬
                elif tag_name in ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div', 'span']:
                    await self._process_text_element(child, mixed_content)
                
                # ë³µí•© ìš”ì†ŒëŠ” ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
                else:
                    await self._traverse_elements(child, mixed_content, blog_id, page)
            
        except Exception as e:
            print(f"ìš”ì†Œ íƒìƒ‰ ì˜¤ë¥˜: {e}")

    async def _process_image_element(self, img_element, mixed_content, blog_id):
        """ì´ë¯¸ì§€ ìš”ì†Œ ì²˜ë¦¬"""
        try:
            src = await img_element.get_attribute('src')
            alt = await img_element.get_attribute('alt') or "ì´ë¯¸ì§€"
            
            if src and ('postfiles.pstatic.net' in src or 'blogfiles.naver.net' in src):
                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                images_dir = Path(f"images/{blog_id}")
                images_dir.mkdir(parents=True, exist_ok=True)
                
                image_count = len([item for item in mixed_content if item['type'] == 'image']) + 1
                filename = await self._download_image(src, images_dir, f"{blog_id}_image_{image_count}")
                
                if filename:
                    mixed_content.append({
                        'type': 'image',
                        'src': src,
                        'alt': alt,
                        'local_path': f"images/{blog_id}/{filename}",
                        'filename': filename
                    })
                    print(f"ğŸ“· ì´ë¯¸ì§€ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬ ì™„ë£Œ: {filename}")
                    
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ìš”ì†Œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    async def _process_text_element(self, element, mixed_content):
        """í…ìŠ¤íŠ¸ ìš”ì†Œ ì²˜ë¦¬"""
        try:
            text = await element.inner_text()
            if text and text.strip():
                cleaned_text = self._clean_text_segment(text.strip())
                if cleaned_text and len(cleaned_text) > 5:
                    mixed_content.append({
                        'type': 'text',
                        'content': cleaned_text
                    })
                    
        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ìš”ì†Œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    def _clean_text_segment(self, text):
        """ê°œë³„ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ì •ë¦¬"""
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ UI ìš”ì†Œ í•„í„°ë§
        skip_patterns = [
            r'^ê³µê°\s*\d*$',
            r'^ëŒ“ê¸€\s*\d*$',
            r'^ìŠ¤í¬ë©$',
            r'^ì´ì›ƒì¶”ê°€$',
            r'^êµ¬ë…$',
            r'^ì¢‹ì•„ìš”$',
            r'^ì´ì „ê¸€$',
            r'^ë‹¤ìŒê¸€$',
            r'^ëª©ë¡$',
            r'^ë¸”ë¡œê·¸$',
            r'^NAVER$',
            r'^ë¸”ë¡œê·¸ ê²€ìƒ‰$',
            r'^ë©”ë‰´ ë°”ë¡œê°€ê¸°$',
            r'^ë³¸ë¬¸ ë°”ë¡œê°€ê¸°$',
        ]
        
        for pattern in skip_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                return None
        
        return text

    def _convert_to_markdown(self, mixed_content):
        """í˜¼í•© ì½˜í…ì¸ ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜"""
        markdown_parts = []
        
        for item in mixed_content:
            if item['type'] == 'text':
                markdown_parts.append(item['content'])
                markdown_parts.append('')  # ë¹ˆ ì¤„ ì¶”ê°€
            elif item['type'] == 'image':
                markdown_parts.append(f"![{item['alt']}]({item['local_path']})")
                markdown_parts.append('')  # ë¹ˆ ì¤„ ì¶”ê°€
        
        return '\n'.join(markdown_parts).strip()
    
    async def _extract_date(self, page):
        """ì‘ì„±ì¼ ì¶”ì¶œ"""
        try:
            # í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
            page_content = await page.content()
            
            # í•œêµ­ì–´ ë‚ ì§œ íŒ¨í„´ë“¤
            date_patterns = [
                r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
                r'(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})',
                r'(\d{4})\.(\d{1,2})\.(\d{1,2})',
            ]
            
            for pattern in date_patterns:
                matches = re.findall(pattern, page_content)
                if matches:
                    # ì²« ë²ˆì§¸ ë§¤ì¹˜ë¥¼ ì‚¬ìš© (ë³´í†µ ì‘ì„±ì¼)
                    year, month, day = matches[0]
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            return datetime.now().strftime('%Y-%m-%d')
            
        except Exception as e:
            print(f"ë‚ ì§œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return datetime.now().strftime('%Y-%m-%d')
    
    def _clean_content(self, content):
        """ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not content:
            return ""
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        content = re.sub(r'\n\s*\n', '\n\n', content)  # ì—°ì†ëœ ë¹ˆ ì¤„ì„ ë‘ ì¤„ë¡œ
        content = re.sub(r'[ \t]+', ' ', content)      # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        
        lines = content.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            
            # ë¹ˆ ì¤„ì€ ìœ ì§€
            if not line:
                cleaned_lines.append('')
                continue
            
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ UI ìš”ì†Œ ì œê±°
            skip_patterns = [
                r'^ê³µê°\s*\d*$',
                r'^ëŒ“ê¸€\s*\d*$',
                r'^ìŠ¤í¬ë©$',
                r'^ì´ì›ƒì¶”ê°€$',
                r'^êµ¬ë…$',
                r'^ì¢‹ì•„ìš”$',
                r'^ì´ì „ê¸€$',
                r'^ë‹¤ìŒê¸€$',
                r'^ëª©ë¡$',
                r'^ë¸”ë¡œê·¸$',
                r'^ì¹´í…Œê³ ë¦¬$',
                r'^íƒœê·¸$',
                r'^ë°©ë¬¸ì$',
                r'ë„¤ì´ë²„.*ë¸”ë¡œê·¸',
                r'^Copyright',
                r'^\[.*\]$',  # ëŒ€ê´„í˜¸ë¡œ ê°ì‹¸ì§„ í…ìŠ¤íŠ¸
            ]
            
            should_skip = False
            for pattern in skip_patterns:
                if re.match(pattern, line, re.IGNORECASE):
                    should_skip = True
                    break
            
            if not should_skip:
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines).strip()



    async def _download_image(self, url, images_dir, base_filename):
        """ì´ë¯¸ì§€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            # URLì—ì„œ íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
            parsed_url = urlparse(url)
            path = parsed_url.path
            ext = os.path.splitext(path)[1] or '.jpg'
            
            filename = f"{base_filename}{ext}"
            filepath = images_dir / filename
            
            # aiohttpë¡œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        content = await response.read()
                        filepath.write_bytes(content)
                        return filename
            
            return None
            
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {url}: {e}")
            return None
    
    async def crawl_url(self, url, output_format='markdown'):
        """ë‹¨ì¼ URL í¬ë¡¤ë§"""
        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹œì‘ (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ)
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )
            
            try:
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                )
                
                page = await context.new_page()
                
                print(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
                
                # í˜ì´ì§€ ë¡œë“œ
                await page.goto(url, wait_until='networkidle', timeout=30000)
                
                # ì½˜í…ì¸  ì¶”ì¶œ
                result = await self.extract_content(page, url)
                
                if result:
                    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {result['title']}")
                    
                    # íŒŒì¼ë¡œ ì €ì¥
                    if output_format == 'markdown':
                        await self._save_as_markdown(result)
                    elif output_format == 'json':
                        await self._save_as_json(result)
                    
                    return result
                else:
                    print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
                    return None
                    
            finally:
                await browser.close()
    
    async def _save_as_markdown(self, data):
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥"""
        filename = f"naver_blog_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        markdown_content = f"""---
title: "{data['title']}"
author: "{data['author']}"
url: "{data['url']}"
published_date: "{data['published_date']}"
scraped_at: "{data['scraped_at']}"
content_length: {data['content_length']}
word_count: {data['word_count']}
image_count: {data.get('image_count', 0)}
---

# {data['title']}

**ì‘ì„±ì**: {data['author']}  
**ì‘ì„±ì¼**: {data['published_date']}  
**ì¶œì²˜**: {data['url']}
**ì´ë¯¸ì§€ ìˆ˜**: {data.get('image_count', 0)}ê°œ

---

{data['content']}
"""
        
        Path(filename).write_text(markdown_content, encoding='utf-8')
        print(f"ğŸ“„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥: {filename}")
    
    async def _save_as_json(self, data):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        filename = f"naver_blog_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        Path(filename).write_text(
            json.dumps(data, ensure_ascii=False, indent=2), 
            encoding='utf-8'
        )
        print(f"ğŸ“„ JSON íŒŒì¼ ì €ì¥: {filename}")


async def main():
    parser = argparse.ArgumentParser(description='ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬')
    parser.add_argument('url', help='í¬ë¡¤ë§í•  ë„¤ì´ë²„ ë¸”ë¡œê·¸ URL')
    parser.add_argument('--format', choices=['markdown', 'json'], 
                       default='markdown', help='ì¶œë ¥ í˜•ì‹ (ê¸°ë³¸ê°’: markdown)')
    
    args = parser.parse_args()
    
    # URL ê²€ì¦
    if 'blog.naver.com' not in args.url:
        print("âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        return
    
    crawler = NaverBlogCrawler()
    result = await crawler.crawl_url(args.url, args.format)
    
    if result:
        print("\n=== í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ===")
        print(f"ì œëª©: {result['title']}")
        print(f"ì‘ì„±ì: {result['author']}")
        print(f"ì‘ì„±ì¼: {result['published_date']}")
        print(f"ë‚´ìš© ê¸¸ì´: {result['content_length']}ì")
        print(f"ë‹¨ì–´ ìˆ˜: {result['word_count']}ê°œ")
        print(f"ì´ë¯¸ì§€ ìˆ˜: {result.get('image_count', 0)}ê°œ")
        print("âœ… í¬ë¡¤ë§ ì„±ê³µ!")
    else:
        print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")


if __name__ == "__main__":
    asyncio.run(main()) 
